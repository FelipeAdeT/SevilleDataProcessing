{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the results of Training (K-fold Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of training (and its evaluation) will depend on how the data was split into training and testing sets. In this worksheet, we use repeated random subsampling to assess the performance of our trained model.\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Cross-validation_(statistics)): \n",
    ">In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[11] but in general k remains an unfixed parameter.\n",
    "\n",
    "More information available [here](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "\n",
    "For us, measuring performance with different samples is important because of the wide variation in the data: texts vary widely in length, in type, and in transcription conventions. We cannot tell clearly whether the performance of the model, when measured only once, reflects an improvement in the model through training or whether it is the result of the division into training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "from spacy.lang.es import Spanish \n",
    "from spacy.scorer import Scorer\n",
    "from spacy.language import GoldParse\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import plac\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tagged Data from JSON file\n",
    "with open('AMSTrainingII_SF.json', 'r', encoding='utf-8') as fp2:\n",
    "    TAGGED_DATA = json.load(fp2)\n",
    "    \n",
    "TD_np = np.array(TAGGED_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy has a built-in function for evaluating a model's performance using the [command line](https://spacy.io/api/cli#evaluate), but alternatively you can define a function like the one below. It takes the NER model and examples that you input and returns several metrics:\n",
    "        - UAS (Unlabelled Attachment Score) \n",
    "        - LAS (Labelled Attachment Score)\n",
    "        - ents_p\n",
    "        - ents_r\n",
    "        - ents_f\n",
    "        - tags_acc\n",
    "        - token_acc\n",
    "\n",
    "[According](https://github.com/explosion/spaCy/issues/2405) to one of the creators of Spacy, \n",
    ">The UAS and LAS are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    ">ents_p, ents_r, ents_f are the precision, recall and fscore for the NER task.\n",
    ">tags_acc is the POS tagging accuracy.\n",
    ">token_acc seems to be the precision for token segmentation.\n",
    "\n",
    "The key metrics for this task are the precision, recall and f-score.\n",
    "**Precision** (ents_p) is the ratio of correctly-labeled entities out of all the entities labeled. (True Positive/(True Positive+False Positive)).\n",
    "**Recall**  (ents_r) is the ratio of correctly-labeled entities out of all true entities (True Positive/(True Positive+False Negative)). The F-score is the mean of both values.  \n",
    "\n",
    "These metrics all appear averaged out through all the entity types (labels) and then detailed for each label in particular. We want these values to be as close as possible to 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the evaluate function\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for sents, ents in examples:\n",
    "        doc_gold = ner_model.make_doc(sents)\n",
    "        gold = GoldParse(doc_gold, entities=ents['entities'])\n",
    "        pred_value = ner_model(sents)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the spacy model and split the data into the n batches that we will use in the cross-validation. In this procedure, we will train the model n-1 times, reserving one fold for testing the model each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy Model\n",
    "nlp= spacy.load('es_core_news_ml_EMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters of k-fold split (5 batches, with random shuffle, set seed = 2)\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= kf.split(TD_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also create a dataframe to store the results of each training, with the evaluation scores for each label type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data = pd.DataFrame(columns=columns)\n",
    "eval_data = eval_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop for each set of training data excluding one fold and evaluate the results, storing these in our dataframe. We are using a copy of the NLP model because we want the training to start afresh for each set of training data. Otherwise, the model would be trained on all the data including the test data, leading to the model overperforming on the tagged data compared to new samples that we are interested in tagging later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3870.829282823463}\n",
      "Losses {'ner': 2763.04435870662}\n",
      "Losses {'ner': 1476.2307875847337}\n",
      "Losses {'ner': 773.8409690879708}\n",
      "Losses {'ner': 180.4159515845463}\n",
      "Losses {'ner': 27.676898249483802}\n",
      "Losses {'ner': 14.246774299018416}\n",
      "Losses {'ner': 6.202203421710105}\n",
      "Losses {'ner': 0.12942503925426443}\n",
      "Losses {'ner': 0.27166528410552937}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3933.884142343929}\n",
      "Losses {'ner': 2291.4247039360093}\n",
      "Losses {'ner': 1121.7010050259992}\n",
      "Losses {'ner': 342.57781070640607}\n",
      "Losses {'ner': 260.33463196042584}\n",
      "Losses {'ner': 40.44939581508031}\n",
      "Losses {'ner': 17.991775464927834}\n",
      "Losses {'ner': 0.6208079673153009}\n",
      "Losses {'ner': 0.004879211893098394}\n",
      "Losses {'ner': 0.005636297302901938}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4092.368336050526}\n",
      "Losses {'ner': 2428.0468781576}\n",
      "Losses {'ner': 1279.1446418118621}\n",
      "Losses {'ner': 438.7236478671269}\n",
      "Losses {'ner': 197.8198385347393}\n",
      "Losses {'ner': 48.96829818833708}\n",
      "Losses {'ner': 16.633989205062946}\n",
      "Losses {'ner': 0.6730541260740408}\n",
      "Losses {'ner': 0.24261554933569401}\n",
      "Losses {'ner': 0.018371099717744258}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4045.306469922817}\n",
      "Losses {'ner': 2404.9514029736038}\n",
      "Losses {'ner': 1499.7526436548756}\n",
      "Losses {'ner': 566.0273877225928}\n",
      "Losses {'ner': 125.58810682910273}\n",
      "Losses {'ner': 59.50530844948313}\n",
      "Losses {'ner': 20.238625950698346}\n",
      "Losses {'ner': 11.445776533489672}\n",
      "Losses {'ner': 1.6540043926710177}\n",
      "Losses {'ner': 2.250796257274562}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3498.829717476414}\n",
      "Losses {'ner': 1995.7019147103404}\n",
      "Losses {'ner': 1095.0194388449631}\n",
      "Losses {'ner': 371.8198865141883}\n",
      "Losses {'ner': 122.3746062087221}\n",
      "Losses {'ner': 44.52421632393097}\n",
      "Losses {'ner': 15.987618927356936}\n",
      "Losses {'ner': 4.788533532276276}\n",
      "Losses {'ner': 2.2928582122737917}\n",
      "Losses {'ner': 0.0036807689633403676}\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp1 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp1.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    #ner.add_label(\"OBJ\")\n",
    "    #ner.add_label(\"MON\")\n",
    "    #ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp1.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp1.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp1.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp1,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    #ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    #oblist = list(ev_obj[0].values())\n",
    "    #newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data=eval_data.append(newrow1,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow2,ignore_index=True)\n",
    "    #eval_data=eval_data.append(newrow3,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow4,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow5,ignore_index=True)\n",
    "    eval_data=eval_data.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the contents of our evaluation dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ents_p     ents_r     ents_f label\n",
      "0   70.588235  77.419355  73.846154  DATE\n",
      "1   73.717949  86.466165  79.584775   MON\n",
      "2   57.746479  56.944444  57.342657   ORG\n",
      "3   89.487871  89.972900  89.729730   PER\n",
      "4   80.952381  78.461538  79.687500   LOC\n",
      "5   90.000000  78.947368  84.112150  DATE\n",
      "6   71.428571  73.170732  72.289157   MON\n",
      "7   65.217391  46.153846  54.054054   ORG\n",
      "8   92.248062  88.805970  90.494297   PER\n",
      "9   90.647482  86.896552  88.732394   LOC\n",
      "10  86.153846  67.469880  75.675676  DATE\n",
      "11  75.490196  82.795699  78.974359   MON\n",
      "12  61.290323  44.705882  51.700680   ORG\n",
      "13  92.413793  85.079365  88.595041   PER\n",
      "14  81.595092  88.079470  84.713376   LOC\n",
      "15  79.629630  72.881356  76.106195  DATE\n",
      "16  76.666667  84.146341  80.232558   MON\n",
      "17  55.555556  46.666667  50.724638   ORG\n",
      "18  89.430894  90.659341  90.040928   PER\n",
      "19  76.433121  73.619632  75.000000   LOC\n",
      "20  84.745763  90.909091  87.719298  DATE\n",
      "21  43.037975  80.952381  56.198347   MON\n",
      "22  47.826087  50.000000  48.888889   ORG\n",
      "23  93.486590  94.941634  94.208494   PER\n",
      "24  88.489209  86.013986  87.234043   LOC\n"
     ]
    }
   ],
   "source": [
    "print(eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can create estimates of performance averaged over all the trials, providing a better estimate of each measurement with its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure mean and standard deviation of f, p and r scores for each label \n",
    "a = eval_data.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f                ents_p                ents_r          \n",
      "            mean        std       mean        std       mean       std\n",
      "label                                                                 \n",
      "DATE   79.491894   6.060895  82.223495   7.489842  77.525410  8.715461\n",
      "LOC    83.073463   5.671613  83.623457   5.830144  82.614236  6.283174\n",
      "MON    73.455839  10.162726  68.068272  14.131026  81.506264  5.074967\n",
      "ORG    52.542184   3.266820  57.527167   6.544983  48.894168  4.900068\n",
      "PER    90.613698   2.128453  91.413442   1.846182  89.891842  3.550883\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the different labels perform consistently at the levels printed above. The PER and LOC labels are perhaps the most useful, whereas the others can still be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Spelling Normalization\n",
    "\n",
    "We can apply the evaluation above to a model trained with text whose spelling has been normalized, thus evaluating whether the inclusion of a normalization dictionary improves training results.\n",
    "\n",
    "To apply the spelling normalization, we create a pipeline component that modifies the NORM attribute of each token according to a dictionary we provide. Spacy does not modify any text supplied permanently, this is the way they provide for correcting for spelling variation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Norm Exceptions from JSON file\n",
    "with open('normalizeddict.json', 'r', encoding='utf-8') as fp3:\n",
    "    NORM_EXCEPTIONS = json.load(fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These steps are all addressed in more detail in another notebook, \"Adding a Custom Pipeline Component in Spacy\"\n",
    "\n",
    "#Define and add pipeline component that updates .norm attribute\n",
    "\n",
    "def add_custom_norms(doc):\n",
    "    for token in doc:\n",
    "        if token.text in NORM_EXCEPTIONS:\n",
    "            token.norm_ = NORM_EXCEPTIONS[token.text]\n",
    "    return doc\n",
    "\n",
    "#Add component to the pipeline\n",
    "\n",
    "nlp.add_pipe(add_custom_norms, first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a new blank dataframe with columns for the information we are interested in\n",
    "\n",
    "columns=['ents_p', 'ents_r', 'ents_f', 'label']\n",
    "eval_data2 = pd.DataFrame(columns=columns)\n",
    "eval_data2 = eval_data2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ents_p</th>\n",
       "      <th>ents_r</th>\n",
       "      <th>ents_f</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ents_p, ents_r, ents_f, label]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3893.4023136952587}\n",
      "Losses {'ner': 2581.9710635599113}\n",
      "Losses {'ner': 1523.733720502525}\n",
      "Losses {'ner': 628.7234550421648}\n",
      "Losses {'ner': 187.32588130995907}\n",
      "Losses {'ner': 49.037406406120894}\n",
      "Losses {'ner': 24.944327399552055}\n",
      "Losses {'ner': 1.8219491690634046}\n",
      "Losses {'ner': 0.02951674448542816}\n",
      "Losses {'ner': 0.0730018346335396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4304.295866752207}\n",
      "Losses {'ner': 2853.9652720915055}\n",
      "Losses {'ner': 968.0967388995275}\n",
      "Losses {'ner': 315.7326493885426}\n",
      "Losses {'ner': 123.62306165866235}\n",
      "Losses {'ner': 10.214118412095203}\n",
      "Losses {'ner': 1.642406302719747}\n",
      "Losses {'ner': 0.07822679792910586}\n",
      "Losses {'ner': 2.661344697384856}\n",
      "Losses {'ner': 0.09041250821899786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3951.276671785179}\n",
      "Losses {'ner': 2164.780988466331}\n",
      "Losses {'ner': 775.9616581189613}\n",
      "Losses {'ner': 407.6480709102726}\n",
      "Losses {'ner': 81.59152221909333}\n",
      "Losses {'ner': 17.358426573759935}\n",
      "Losses {'ner': 19.85774139467456}\n",
      "Losses {'ner': 3.454616478935028}\n",
      "Losses {'ner': 0.18575660293935267}\n",
      "Losses {'ner': 2.660929280552897}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3991.850618977065}\n",
      "Losses {'ner': 2765.8190093470216}\n",
      "Losses {'ner': 909.7052457272707}\n",
      "Losses {'ner': 306.5152535692142}\n",
      "Losses {'ner': 112.45289561134007}\n",
      "Losses {'ner': 29.137735492815107}\n",
      "Losses {'ner': 10.020323340201747}\n",
      "Losses {'ner': 13.744271439609456}\n",
      "Losses {'ner': 1.4102048315957108}\n",
      "Losses {'ner': 0.34048508198137556}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Felipe/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W020] Unnamed vectors. This won't allow multiple vectors models to be loaded. (Shape: (20000, 50))\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4659.727620554147}\n",
      "Losses {'ner': 2441.2476337305056}\n",
      "Losses {'ner': 1480.1646121630135}\n",
      "Losses {'ner': 658.9802001129704}\n",
      "Losses {'ner': 286.54711319366584}\n",
      "Losses {'ner': 92.7157910272058}\n",
      "Losses {'ner': 18.969198043135027}\n",
      "Losses {'ner': 0.05823734139601938}\n",
      "Losses {'ner': 6.763841825655848}\n",
      "Losses {'ner': 0.006211504160673457}\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Model trained with EMS dictionary\n",
    "\n",
    "for train_index, test_index in split:\n",
    "    \n",
    "    #Generate training and test data\n",
    "    traindata = TD_np[train_index]\n",
    "    testdata = TD_np[test_index]\n",
    "    \n",
    "    #Load the model to be trained (save separately, because we do not want to repeatedly retrain the same model)\n",
    "    nlp2 = deepcopy(nlp)\n",
    "    \n",
    "    #Create object for retrieving the NER pipeline component\n",
    "    ner=nlp2.get_pipe(\"ner\")\n",
    "\n",
    "    #Generate new labels for the NER component (if you wish to create new labels)\n",
    "    #ner.add_label(\"OBJ\")\n",
    "    #ner.add_label(\"MON\")\n",
    "    #ner.add_label(\"DATE\")\n",
    "\n",
    "    #This piece of code creates a loop in which we train the model, but only for the NER component (disabling the tagger and the parser, which we are not using here).\n",
    "    with nlp2.disable_pipes('tagger','parser'):\n",
    "    #Here we resume training, alternatively you could begin_training if you are starting on a new model.\n",
    "        optimizer= nlp2.resume_training()\n",
    "    #Would need to figure this out, they are the sizes for the minibatching\n",
    "        sizes = compounding(1.0, 4.0, 1.001)\n",
    "    #This loops the training mechanism 10 times, randomly shuffling the training data and creating mini-batches from which the algorithm learns to label. Each time a batch is processed, the model is updated.\n",
    "        for itn in range(10):\n",
    "            random.shuffle(traindata)\n",
    "            batches = minibatch(traindata, size=sizes)\n",
    "            losses = {}\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp2.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    results = evaluate(nlp2,testdata)\n",
    "    evaluation= dict((k, results[k]) for k in ['ents_per_type'] \n",
    "                                        if k in results)\n",
    "    \n",
    "    ev_date = [val.get('DATE') for val in evaluation.values()]\n",
    "    ev_mon= [val.get('MON') for val in evaluation.values()]\n",
    "    #ev_obj= [val.get('OBJ') for val in evaluation.values()]\n",
    "    ev_org= [val.get('ORG') for val in evaluation.values()]\n",
    "    ev_per= [val.get('PER') for val in evaluation.values()]\n",
    "    ev_loc= [val.get('LOC') for val in evaluation.values()]\n",
    "    \n",
    "    dlist = list(ev_date[0].values())\n",
    "    newrow1= {'ents_p': dlist[0],'ents_r': dlist[1],'ents_f':dlist[2],'label':'DATE'}\n",
    "    \n",
    "    mlist = list(ev_mon[0].values())\n",
    "    newrow2= {'ents_p': mlist[0],'ents_r':mlist[1],'ents_f':mlist[2],'label':'MON'}\n",
    "                  \n",
    "    #oblist = list(ev_obj[0].values())\n",
    "    #newrow3= {'ents_p':oblist[0],'ents_r':oblist[1],'ents_f':oblist[2],'label':'OBJ'}\n",
    "                  \n",
    "    orlist = list(ev_org[0].values())\n",
    "    newrow4= {'ents_p':orlist[0],'ents_r':orlist[1],'ents_f':orlist[2],'label':'ORG'}\n",
    "                  \n",
    "    plist = list(ev_per[0].values())\n",
    "    newrow5= {'ents_p':plist[0],'ents_r':plist[1],'ents_f':plist[2],'label':'PER'}\n",
    "                  \n",
    "    llist = list(ev_loc[0].values())\n",
    "    newrow6= {'ents_p':llist[0],'ents_r':llist[1],'ents_f':llist[2],'label':'LOC'}\n",
    "                  \n",
    "    eval_data2=eval_data2.append(newrow1,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow2,ignore_index=True)\n",
    "    #eval_data2=eval_data2.append(newrow3,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow4,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow5,ignore_index=True)\n",
    "    eval_data2=eval_data2.append(newrow6,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b= eval_data2.groupby('label').agg({'ents_f':['mean','std'],'ents_p':['mean','std'],'ents_r':['mean','std']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we print the statistics for the training with (b) and without (a) spelling normalization. As can be seen, there is a slight improvement on most measurements (as well as a reduction in variability) when we normalize spelling. \n",
    "\n",
    "This measurement shows null performance of the DATE and OBJ labels; this must be reviewed, but may be because of the way the data was shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bca0e2660b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ents_f               ents_p                ents_r          \n",
      "            mean       std       mean        std       mean       std\n",
      "label                                                                \n",
      "DATE   80.009995  5.963633  82.810063   6.841508  77.998465  9.247658\n",
      "LOC    82.947133  4.536340  83.798343   7.382361  82.476082  4.633619\n",
      "MON    72.658621  9.496365  67.678875  13.748218  79.963806  4.016407\n",
      "ORG    51.784561  5.077312  59.120165   6.512367  47.191741  9.022116\n",
      "PER    89.491461  4.065115  89.947766   3.294965  89.067978  4.987185\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
